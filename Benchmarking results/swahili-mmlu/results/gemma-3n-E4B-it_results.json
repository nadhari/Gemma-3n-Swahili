{
  "model_name": "gemma-3n-E4B-it",
  "model_id": "unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit",
  "overall_accuracy": 0.246,
  "subject_accuracies": {
    "college_biology": 0.375,
    "astronomy": 0.25,
    "high_school_government_and_politics": 0.375,
    "professional_medicine": 0.25,
    "high_school_biology": 0.125,
    "anatomy": 0.25,
    "college_chemistry": 0.25,
    "management": 0.3333333333333333,
    "nutrition": 0.375,
    "us_foreign_policy": 0.125,
    "jurisprudence": 0.0,
    "world_religions": 0.125,
    "high_school_statistics": 0.1111111111111111,
    "college_medicine": 0.4444444444444444,
    "clinical_knowledge": 0.18181818181818182,
    "machine_learning": 0.25,
    "high_school_mathematics": 0.25,
    "public_relations": 0.25,
    "medical_genetics": 0.0,
    "miscellaneous": 0.09090909090909091,
    "high_school_psychology": 0.18181818181818182,
    "virology": 0.1111111111111111,
    "high_school_microeconomics": 0.3,
    "high_school_us_history": 0.4,
    "prehistory": 0.4444444444444444,
    "college_computer_science": 0.25,
    "human_aging": 0.0,
    "college_physics": 0.25,
    "sociology": 0.3333333333333333,
    "high_school_world_history": 0.2222222222222222,
    "high_school_european_history": 0.75,
    "econometrics": 0.375,
    "professional_law": 0.09090909090909091,
    "moral_scenarios": 0.09090909090909091,
    "business_ethics": 0.0,
    "marketing": 0.4444444444444444,
    "high_school_macroeconomics": 0.0,
    "high_school_physics": 0.0,
    "logical_fallacies": 0.2222222222222222,
    "formal_logic": 0.0,
    "philosophy": 0.375,
    "human_sexuality": 0.5,
    "security_studies_test-sw-KE.csv": 0.2222222222222222,
    "high_school_computer_science": 0.3333333333333333,
    "electrical_engineering": 0.5555555555555556,
    "high_school_geography": 0.4,
    "computer_security": 0.125,
    "abstract_algebra": 0.25,
    "professional_psychology": 0.2222222222222222,
    "college_mathematics_test.csv_sw-KE.csv": 0.5,
    "global_facts": 0.5,
    "moral_disputes": 0.2222222222222222,
    "international_law": 0.125,
    "professional_accounting": 0.3,
    "high_school_chemistry": 0.25,
    "conceptual_physics": 0.25,
    "elementary_mathematics": 0.2222222222222222
  },
  "subject_counts": {
    "college_biology": 8,
    "astronomy": 8,
    "high_school_government_and_politics": 8,
    "professional_medicine": 8,
    "high_school_biology": 8,
    "anatomy": 8,
    "college_chemistry": 8,
    "management": 9,
    "nutrition": 8,
    "us_foreign_policy": 8,
    "jurisprudence": 8,
    "world_religions": 8,
    "high_school_statistics": 9,
    "college_medicine": 9,
    "clinical_knowledge": 11,
    "machine_learning": 8,
    "high_school_mathematics": 8,
    "public_relations": 8,
    "medical_genetics": 8,
    "miscellaneous": 11,
    "high_school_psychology": 11,
    "virology": 9,
    "high_school_microeconomics": 10,
    "high_school_us_history": 10,
    "prehistory": 9,
    "college_computer_science": 8,
    "human_aging": 10,
    "college_physics": 8,
    "sociology": 9,
    "high_school_world_history": 9,
    "high_school_european_history": 8,
    "econometrics": 8,
    "professional_law": 11,
    "moral_scenarios": 11,
    "business_ethics": 9,
    "marketing": 9,
    "high_school_macroeconomics": 11,
    "high_school_physics": 8,
    "logical_fallacies": 9,
    "formal_logic": 8,
    "philosophy": 8,
    "human_sexuality": 8,
    "security_studies_test-sw-KE.csv": 9,
    "high_school_computer_science": 9,
    "electrical_engineering": 9,
    "high_school_geography": 10,
    "computer_security": 8,
    "abstract_algebra": 8,
    "professional_psychology": 9,
    "college_mathematics_test.csv_sw-KE.csv": 8,
    "global_facts": 8,
    "moral_disputes": 9,
    "international_law": 8,
    "professional_accounting": 10,
    "high_school_chemistry": 8,
    "conceptual_physics": 8,
    "elementary_mathematics": 9
  },
  "total_examples": 13757,
  "correct_predictions": 3384,
  "evaluation_time_seconds": 179.588225,
  "timestamp": "2025-08-07T16:26:03.292271",
  "predictions": [
    {
      "idx": 0,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.04820830002427101,
        0.013386962004005909,
        0.7084158062934875,
        0.2299889326095581
      ],
      "subject": "college_biology"
    },
    {
      "idx": 1,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.16633707284927368,
        0.03950851783156395,
        0.5805734395980835,
        0.21358104050159454
      ],
      "subject": "astronomy"
    },
    {
      "idx": 2,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.04683033376932144,
        0.011840562336146832,
        0.6881667375564575,
        0.2531624138355255
      ],
      "subject": "high_school_government_and_politics"
    },
    {
      "idx": 3,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.11441817134618759,
        0.026340557262301445,
        0.6584311127662659,
        0.20081014931201935
      ],
      "subject": "professional_medicine"
    },
    {
      "idx": 4,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.0227894876152277,
        0.015662970021367073,
        0.8033571243286133,
        0.15819039940834045
      ],
      "subject": "high_school_biology"
    },
    {
      "idx": 5,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.03905844688415527,
        0.010189005173742771,
        0.671026885509491,
        0.2797256112098694
      ],
      "subject": "anatomy"
    },
    {
      "idx": 6,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.23145435750484467,
        0.00817133765667677,
        0.6697353720664978,
        0.09063882380723953
      ],
      "subject": "college_chemistry"
    },
    {
      "idx": 7,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.10966341942548752,
        0.01351318508386612,
        0.8103091716766357,
        0.06651422381401062
      ],
      "subject": "management"
    },
    {
      "idx": 8,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.08573073893785477,
        0.009618722833693027,
        0.8133906126022339,
        0.0912598967552185
      ],
      "subject": "nutrition"
    },
    {
      "idx": 9,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.06892673671245575,
        0.008232124149799347,
        0.8396995663642883,
        0.0831415206193924
      ],
      "subject": "us_foreign_policy"
    },
    {
      "idx": 10,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.28951576352119446,
        0.031483378261327744,
        0.5757708549499512,
        0.10323002189397812
      ],
      "subject": "jurisprudence"
    },
    {
      "idx": 11,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.11589770764112473,
        0.004783669486641884,
        0.8044894933700562,
        0.07482918351888657
      ],
      "subject": "world_religions"
    },
    {
      "idx": 12,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.006228300742805004,
        0.005163442809134722,
        0.6977449059486389,
        0.2908633351325989
      ],
      "subject": "high_school_statistics"
    },
    {
      "idx": 13,
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.9336265921592712,
        0.0018023233860731125,
        0.04948039725422859,
        0.015090668573975563
      ],
      "subject": "college_medicine"
    },
    {
      "idx": 14,
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "probs": [
        0.09076403826475143,
        0.015287157148122787,
        0.433010995388031,
        0.4609377980232239
      ],
      "subject": "college_medicine"
    },
    {
      "idx": 15,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.031182624399662018,
        0.022813698276877403,
        0.48777833580970764,
        0.4582253396511078
      ],
      "subject": "professional_medicine"
    },
    {
      "idx": 16,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.15399135649204254,
        0.023615341633558273,
        0.7346516847610474,
        0.08774163573980331
      ],
      "subject": "world_religions"
    },
    {
      "idx": 17,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.08847865462303162,
        0.011248798109591007,
        0.8394621014595032,
        0.060810431838035583
      ],
      "subject": "clinical_knowledge"
    },
    {
      "idx": 18,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.3717803657054901,
        0.01850985363125801,
        0.5409372448921204,
        0.06877245754003525
      ],
      "subject": "machine_learning"
    },
    {
      "idx": 19,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.23359179496765137,
        0.0045543150044977665,
        0.6759202480316162,
        0.085933618247509
      ],
      "subject": "high_school_mathematics"
    },
    {
      "idx": 20,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.0032834219746291637,
        0.007634184788912535,
        0.9393011927604675,
        0.04978113994002342
      ],
      "subject": "public_relations"
    },
    {
      "idx": 21,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.17602409422397614,
        0.021023079752922058,
        0.6961888074874878,
        0.10676401853561401
      ],
      "subject": "medical_genetics"
    },
    {
      "idx": 22,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.2482188194990158,
        0.010245214216411114,
        0.6747286915779114,
        0.06680718809366226
      ],
      "subject": "miscellaneous"
    },
    {
      "idx": 23,
      "predicted": "A",
      "correct": "C",
      "is_correct": false,
      "probs": [
        0.5197792649269104,
        0.006750754080712795,
        0.09319128096103668,
        0.38027864694595337
      ],
      "subject": "high_school_psychology"
    },
    {
      "idx": 24,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.11302606761455536,
        0.008715634234249592,
        0.7845563888549805,
        0.09370189905166626
      ],
      "subject": "machine_learning"
    },
    {
      "idx": 25,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.015063626691699028,
        0.010353068821132183,
        0.8224461078643799,
        0.15213723480701447
      ],
      "subject": "college_chemistry"
    },
    {
      "idx": 26,
      "predicted": "D",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.02720523253083229,
        0.006461809389293194,
        0.4530077874660492,
        0.5133250951766968
      ],
      "subject": "virology"
    },
    {
      "idx": 27,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.08928151428699493,
        0.019921397790312767,
        0.7957574725151062,
        0.09503968060016632
      ],
      "subject": "high_school_microeconomics"
    },
    {
      "idx": 28,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.023611079901456833,
        0.022180557250976562,
        0.6282675266265869,
        0.32594090700149536
      ],
      "subject": "high_school_us_history"
    },
    {
      "idx": 29,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.10814725607633591,
        0.008339420892298222,
        0.7052086591720581,
        0.17830467224121094
      ],
      "subject": "high_school_psychology"
    },
    {
      "idx": 30,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.09433584660291672,
        0.027885621413588524,
        0.8149373531341553,
        0.06284122169017792
      ],
      "subject": "prehistory"
    },
    {
      "idx": 31,
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.4583377242088318,
        0.018335815519094467,
        0.27799588441848755,
        0.24533051252365112
      ],
      "subject": "college_computer_science"
    },
    {
      "idx": 32,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.01726008579134941,
        0.005264028441160917,
        0.6084389686584473,
        0.36903688311576843
      ],
      "subject": "high_school_us_history"
    },
    {
      "idx": 33,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.09905099123716354,
        0.009213177487254143,
        0.48754605650901794,
        0.4041898548603058
      ],
      "subject": "human_aging"
    },
    {
      "idx": 34,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.019819453358650208,
        0.002767415950074792,
        0.9549547433853149,
        0.022458383813500404
      ],
      "subject": "college_physics"
    },
    {
      "idx": 35,
      "predicted": "D",
      "correct": "D",
      "is_correct": true,
      "probs": [
        0.0008042408153414726,
        0.0008042408153414726,
        0.18271447718143463,
        0.8156770467758179
      ],
      "subject": "college_chemistry"
    },
    {
      "idx": 36,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.05086301639676094,
        0.014572498388588428,
        0.659601628780365,
        0.2749628722667694
      ],
      "subject": "sociology"
    },
    {
      "idx": 37,
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.9596049189567566,
        0.004109891597181559,
        0.01757577806711197,
        0.018709316849708557
      ],
      "subject": "high_school_world_history"
    },
    {
      "idx": 38,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.037597861140966415,
        0.0061376821249723434,
        0.910912811756134,
        0.045351676642894745
      ],
      "subject": "human_aging"
    },
    {
      "idx": 39,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.007343928795307875,
        0.10459796339273453,
        0.7260537147521973,
        0.1620044857263565
      ],
      "subject": "high_school_european_history"
    },
    {
      "idx": 40,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.07987812906503677,
        0.00526853371411562,
        0.6688113808631897,
        0.24604196846485138
      ],
      "subject": "econometrics"
    },
    {
      "idx": 41,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.06746608763933182,
        0.02993789315223694,
        0.6813833117485046,
        0.22121277451515198
      ],
      "subject": "professional_law"
    },
    {
      "idx": 42,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.11789581924676895,
        0.01030162163078785,
        0.6373387575149536,
        0.23446382582187653
      ],
      "subject": "high_school_government_and_politics"
    },
    {
      "idx": 43,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.31775999069213867,
        0.025880346074700356,
        0.523897647857666,
        0.13246206939220428
      ],
      "subject": "moral_scenarios"
    },
    {
      "idx": 44,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.14255647361278534,
        0.030830001458525658,
        0.4391048550605774,
        0.38750866055488586
      ],
      "subject": "high_school_government_and_politics"
    },
    {
      "idx": 45,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.023564917966723442,
        0.01835237629711628,
        0.830691933631897,
        0.12739074230194092
      ],
      "subject": "business_ethics"
    },
    {
      "idx": 46,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.061908215284347534,
        0.025807183235883713,
        0.7541964650154114,
        0.15808816254138947
      ],
      "subject": "marketing"
    },
    {
      "idx": 47,
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "probs": [
        0.015630090609192848,
        0.007617497351020575,
        0.4427245855331421,
        0.5340278148651123
      ],
      "subject": "clinical_knowledge"
    },
    {
      "idx": 48,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.13652364909648895,
        0.013517671264708042,
        0.6513178944587708,
        0.19864074885845184
      ],
      "subject": "high_school_psychology"
    },
    {
      "idx": 49,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.00888250395655632,
        0.00834434013813734,
        0.9060400724411011,
        0.07673313468694687
      ],
      "subject": "high_school_mathematics"
    },
    {
      "idx": 50,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.0970592051744461,
        0.014884510077536106,
        0.7634297609329224,
        0.12462648004293442
      ],
      "subject": "nutrition"
    },
    {
      "idx": 51,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.07632681727409363,
        0.006669363006949425,
        0.8735141754150391,
        0.04348970949649811
      ],
      "subject": "college_chemistry"
    },
    {
      "idx": 52,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.05347960814833641,
        0.08283083885908127,
        0.7858771085739136,
        0.07781237363815308
      ],
      "subject": "high_school_macroeconomics"
    },
    {
      "idx": 53,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.11478438973426819,
        0.0041810208931565285,
        0.8481482863426208,
        0.03288627788424492
      ],
      "subject": "high_school_physics"
    },
    {
      "idx": 54,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.04854626581072807,
        0.015760665759444237,
        0.8605031371116638,
        0.07518992573022842
      ],
      "subject": "high_school_biology"
    },
    {
      "idx": 55,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.03432299196720123,
        0.014307952485978603,
        0.831570029258728,
        0.11979902535676956
      ],
      "subject": "prehistory"
    },
    {
      "idx": 56,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.07594805210828781,
        0.013197791762650013,
        0.6769176125526428,
        0.23393647372722626
      ],
      "subject": "logical_fallacies"
    },
    {
      "idx": 57,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.03176919370889664,
        0.02324284054338932,
        0.819338321685791,
        0.1256496012210846
      ],
      "subject": "high_school_european_history"
    },
    {
      "idx": 58,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.1023847684264183,
        0.0074167270213365555,
        0.8053185343742371,
        0.08487994968891144
      ],
      "subject": "world_religions"
    },
    {
      "idx": 59,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.14932475984096527,
        0.013889366760849953,
        0.5905908942222595,
        0.24619491398334503
      ],
      "subject": "professional_law"
    },
    {
      "idx": 60,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.10217063128948212,
        0.004778583999723196,
        0.8554642200469971,
        0.037586476653814316
      ],
      "subject": "formal_logic"
    },
    {
      "idx": 61,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.04025592282414436,
        0.0006713310722261667,
        0.9162206053733826,
        0.042852211743593216
      ],
      "subject": "high_school_statistics"
    },
    {
      "idx": 62,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.008438949473202229,
        0.0029164229054003954,
        0.8343123197555542,
        0.1543322652578354
      ],
      "subject": "high_school_macroeconomics"
    },
    {
      "idx": 63,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.041687432676553726,
        0.016843244433403015,
        0.6521011590957642,
        0.2893681228160858
      ],
      "subject": "philosophy"
    },
    {
      "idx": 64,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.2043372094631195,
        0.008174516260623932,
        0.49017947912216187,
        0.29730889201164246
      ],
      "subject": "high_school_psychology"
    },
    {
      "idx": 65,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.014895760454237461,
        0.009617425501346588,
        0.7882589101791382,
        0.18722790479660034
      ],
      "subject": "college_physics"
    },
    {
      "idx": 66,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.09423787146806717,
        0.00993259809911251,
        0.7890440225601196,
        0.10678549855947495
      ],
      "subject": "human_sexuality"
    },
    {
      "idx": 67,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.012869887985289097,
        0.004884858150035143,
        0.6397901773452759,
        0.34245502948760986
      ],
      "subject": "security_studies_test-sw-KE.csv"
    },
    {
      "idx": 68,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.05101706087589264,
        0.011744794435799122,
        0.9042990207672119,
        0.03293909132480621
      ],
      "subject": "college_chemistry"
    },
    {
      "idx": 69,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.09757453203201294,
        0.014056943356990814,
        0.8169815540313721,
        0.07138705253601074
      ],
      "subject": "anatomy"
    },
    {
      "idx": 70,
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.07457577437162399,
        0.015150987543165684,
        0.20271797478199005,
        0.7075552344322205
      ],
      "subject": "machine_learning"
    },
    {
      "idx": 71,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.32053911685943604,
        0.009092994034290314,
        0.598845899105072,
        0.07152194529771805
      ],
      "subject": "philosophy"
    },
    {
      "idx": 72,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.04066229611635208,
        0.0090729845687747,
        0.8693981766700745,
        0.08086663484573364
      ],
      "subject": "high_school_computer_science"
    },
    {
      "idx": 73,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.08894931524991989,
        0.010960706509649754,
        0.7927966117858887,
        0.10729335248470306
      ],
      "subject": "high_school_microeconomics"
    },
    {
      "idx": 74,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.05415613576769829,
        0.009410925209522247,
        0.8471445441246033,
        0.0892883762717247
      ],
      "subject": "electrical_engineering"
    },
    {
      "idx": 75,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.018708176910877228,
        0.00830170325934887,
        0.7954918742179871,
        0.1774982362985611
      ],
      "subject": "security_studies_test-sw-KE.csv"
    },
    {
      "idx": 76,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.2720681428909302,
        0.15501974523067474,
        0.50829017162323,
        0.06462184339761734
      ],
      "subject": "high_school_geography"
    },
    {
      "idx": 77,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.11906851083040237,
        0.052836328744888306,
        0.6436782479286194,
        0.18441690504550934
      ],
      "subject": "high_school_geography"
    },
    {
      "idx": 78,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.19119806587696075,
        0.0071854619309306145,
        0.7562029957771301,
        0.04541352018713951
      ],
      "subject": "college_computer_science"
    },
    {
      "idx": 79,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.23852390050888062,
        0.014779181219637394,
        0.5721890926361084,
        0.17450782656669617
      ],
      "subject": "human_aging"
    },
    {
      "idx": 80,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.0664544552564621,
        0.008188795298337936,
        0.784672737121582,
        0.1406840682029724
      ],
      "subject": "college_medicine"
    },
    {
      "idx": 81,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.038394995033741,
        0.006672049406915903,
        0.8209211826324463,
        0.13401170074939728
      ],
      "subject": "high_school_macroeconomics"
    },
    {
      "idx": 82,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.08099186420440674,
        0.006245421711355448,
        0.8707445859909058,
        0.04201802238821983
      ],
      "subject": "logical_fallacies"
    },
    {
      "idx": 83,
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.8379275798797607,
        0.004130637273192406,
        0.14560997486114502,
        0.012331805191934109
      ],
      "subject": "formal_logic"
    },
    {
      "idx": 84,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.06272105127573013,
        0.014897555112838745,
        0.6743147969245911,
        0.24806655943393707
      ],
      "subject": "nutrition"
    },
    {
      "idx": 85,
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.7505629062652588,
        0.010057548061013222,
        0.20201127231121063,
        0.037368327379226685
      ],
      "subject": "computer_security"
    },
    {
      "idx": 86,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.09774782508611679,
        0.019247688353061676,
        0.49640440940856934,
        0.386600136756897
      ],
      "subject": "professional_medicine"
    },
    {
      "idx": 87,
      "predicted": "C",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.05985094979405403,
        0.03203590214252472,
        0.6638838648796082,
        0.24422922730445862
      ],
      "subject": "virology"
    },
    {
      "idx": 88,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.054651472717523575,
        0.1231589987874031,
        0.6869257688522339,
        0.13526371121406555
      ],
      "subject": "philosophy"
    },
    {
      "idx": 89,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.3746159076690674,
        0.004430007189512253,
        0.5802164673805237,
        0.040737591683864594
      ],
      "subject": "abstract_algebra"
    },
    {
      "idx": 90,
      "predicted": "A",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.47733303904533386,
        0.013124286197125912,
        0.42124491930007935,
        0.08829773962497711
      ],
      "subject": "professional_psychology"
    },
    {
      "idx": 91,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.2395889312028885,
        0.01531640812754631,
        0.6512702107429504,
        0.0938243716955185
      ],
      "subject": "astronomy"
    },
    {
      "idx": 92,
      "predicted": "C",
      "correct": "C",
      "is_correct": true,
      "probs": [
        0.0322093740105629,
        0.011131261475384235,
        0.7330819964408875,
        0.2235773801803589
      ],
      "subject": "prehistory"
    },
    {
      "idx": 93,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.11335378885269165,
        0.004678669851273298,
        0.8375775218009949,
        0.044389981776475906
      ],
      "subject": "formal_logic"
    },
    {
      "idx": 94,
      "predicted": "D",
      "correct": "A",
      "is_correct": false,
      "probs": [
        0.2531192898750305,
        0.19106444716453552,
        0.209843248128891,
        0.34597301483154297
      ],
      "subject": "abstract_algebra"
    },
    {
      "idx": 95,
      "predicted": "A",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.45486029982566833,
        0.06351272761821747,
        0.42730170488357544,
        0.05432531610131264
      ],
      "subject": "clinical_knowledge"
    },
    {
      "idx": 96,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.08148884773254395,
        0.010360143147408962,
        0.6409593820571899,
        0.267191618680954
      ],
      "subject": "high_school_european_history"
    },
    {
      "idx": 97,
      "predicted": "C",
      "correct": "D",
      "is_correct": false,
      "probs": [
        0.07908397912979126,
        0.0071296365931630135,
        0.7987198829650879,
        0.1150665134191513
      ],
      "subject": "high_school_macroeconomics"
    },
    {
      "idx": 98,
      "predicted": "D",
      "correct": "C",
      "is_correct": false,
      "probs": [
        0.0133752366527915,
        0.011803604662418365,
        0.4569869935512543,
        0.5178341269493103
      ],
      "subject": "high_school_us_history"
    },
    {
      "idx": 99,
      "predicted": "C",
      "correct": "B",
      "is_correct": false,
      "probs": [
        0.18280771374702454,
        0.010640684515237808,
        0.5994033813476562,
        0.20714826881885529
      ],
      "subject": "professional_medicine"
    }
  ]
}